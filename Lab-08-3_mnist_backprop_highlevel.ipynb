{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab-08-3_mnist_backprop_highlevel.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO26kzjh3UeSrBKYZkr8ALg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Lab 8-3: MNIST backpropagation(High-Level) - MNIST 데이터셋 분류하기\n","모두를 위한 딥러닝 시즌2의 [`lab-08_4_mnist_back_prop.ipynb`](https://github.com/deeplearningzerotoall/PyTorch/blob/master/lab-08_4_mnist_back_prop.ipynb)에 해당하는 내용이다. 모두딥 시즌2에서는 직접 미분을 하는 과정까지 다루었으나, 여기서는 high level에서 구현하고, 나머지는 따로 다룬다."],"metadata":{"id":"BcYMyoN-Pdz5"}},{"cell_type":"code","source":["#Import\n","import torch\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# for reproducibility\n","torch.manual_seed(777)\n","if device == 'cuda':\n","  torch.cuda.manual_seed_all(777)\n","\n","#parameters\n","learning_rate = 0.5\n","batch_size = 10\n","\n","#load data (MNIST dataset)\n","mnist_train = dsets.MNIST(root='MNIST_data/',\n","                    train=True,\n","                    transform=transforms.ToTensor(),\n","                    download=True)\n","mnist_test = dsets.MNIST(root='MNIST_data/',\n","                    train=False,\n","                    transform=transforms.ToTensor(),\n","                    download=True)\n","\n","#dataset loader\n","data_loader = torch.utils.data.DataLoader(dataset= mnist_train,\n","                                          batch_size= batch_size,\n","                                          shuffle= True,\n","                                          drop_last= True)"],"metadata":{"id":"dley95PfQbbh","executionInfo":{"status":"ok","timestamp":1657562021568,"user_tz":-540,"elapsed":1116,"user":{"displayName":"‍노장한[학생](소프트웨어융합대학 컴퓨터공학부)","userId":"02460474193047331263"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["#model (high level)\n","linear1 = torch.nn.Linear(28 * 28, 30, bias= True).to(device)\n","linear2 = torch.nn.Linear(30, 10, bias= True).to(device)\n","sigmoid = torch.nn.Sigmoid()\n","\n","model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid).to(device)\n","\n","criterion =  torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)"],"metadata":{"id":"uMk8HonkRpq4","executionInfo":{"status":"ok","timestamp":1657562021569,"user_tz":-540,"elapsed":4,"user":{"displayName":"‍노장한[학생](소프트웨어융합대학 컴퓨터공학부)","userId":"02460474193047331263"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["X_test = mnist_test.data.view(-1, 28 * 28).float().to(device)[:1000]\n","Y_test = mnist_test.targets.to(device)[:1000]\n","\n","#train & test\n","i = 0\n","while i < 10001:\n","  for X, Y in data_loader:\n","    i += 1\n","    X = X.view(-1, 28*28).to(device)\n","    Y=Y.to(device)\n","\n","    hypothesis = model(X)\n","    cost = criterion(hypothesis, Y)\n","\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    if i % 1000 == 0:\n","      with torch.no_grad():\n","        hypothesis = model(X_test)\n","        accurate_prediction = (hypothesis.argmax(1) == Y_test).sum()\n","        print(accurate_prediction.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ykUTtm0ETV9V","executionInfo":{"status":"ok","timestamp":1657562044141,"user_tz":-540,"elapsed":22081,"user":{"displayName":"‍노장한[학생](소프트웨어융합대학 컴퓨터공학부)","userId":"02460474193047331263"}},"outputId":"c9231bf9-159d-4cd3-d914-0a6c48da1c82"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["823\n","849\n","866\n","883\n","897\n","884\n","890\n","895\n","908\n","908\n","903\n","900\n"]}]}]}